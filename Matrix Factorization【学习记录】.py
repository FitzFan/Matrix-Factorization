#!/usr/bin/env python
#coding:utf-8

"""
1、SVD初探
- https://yanyiwu.com/work/2012/09/10/SVD-application-in-recsys.html

2、SVD属于基于协同过滤进行推荐的派系，即“希望预测目标用户对其他未评分物品的评分，进而将评分高的物品推荐给目标用户。”
- 概览总结的还可以：https://www.cnblogs.com/pinard/p/6351319.html
- 需要弄懂以下SVD算法：
-- 传统SVD 
	1. 借助其数学理论，给ML指出了一条不错的路子！~
	2. **PS：在ML中的SVD，被称为是Latent Factor Model。
	3. **ML中玩SVD进行推荐的本质：矩阵分解的思路是把评分矩阵通过分解，用一个低秩的矩阵来逼近原来的评分矩阵，逼近的目标就是让预测的矩阵和原来的矩阵之间的误差平方最小。（矩阵分解一般不用数学上直接分解的办法，尽管分解出来的精度很高，但是效率实在太低！矩阵分解往往会转化为一个优化问题，通过迭代求局部最优解。）
-- FunkSVD
	1. FunkSVD是在传统SVD面临计算效率问题时提出来的，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还面临稀疏的问题，那么我们能不能避开稀疏问题，同时只分解成两个矩阵呢。
	2. 思路很简单：用MSE作为Loss Function，来寻找最终的分解矩阵P和Q。求解方法选用梯度下降，
	3. 为了达到更好的效果，一般会加入L2正则项
	4. ** FunkSVD是一般在ML中使用的SVD。因为传统数学意义的SVD的分解形式意义不大，ML中的SVD只不过借鉴了SVD分解形式：R=U*S*R，通过最优化方法进行模型拟合，求得R=U*V
-- BiasSVD
	- 就是在FunkSVD的基础上加入了Baseline Predictors.[为什么要加入：user_i对item_j的评分是3分，如果mean_rate是1.2，那么3分就很高。如果mean_rate是4.7，那么3分就很低。]
	- http://www.cnblogs.com/Xnice/p/4522671.html
-- SVD++
	1. https://medium.com/@danjtchen/svd-%E6%8E%A8%E8%96%A6%E7%B3%BB%E7%B5%B1-%E5%8E%9F%E7%90%86-c72c2e35af9c
	2. 考虑隐式反馈。隐式反馈的形式很多。一般而言，会选择数据比较稠密且容易收集的特征作为隐式反馈的特征，比如浏览行为、时间咨询，或者其它一些看似无关的特征。
	3. 使用方式：
		- 用户i对物品j的兴趣程度 = 用户i对物品j的显式反馈(大概率缺失) + 用户i的一系列隐式反馈在物品j上表现；
		- 定义一组关于item的隐式反馈特征：比如用户u对每一个item的浏览时间；因为是关于item的特征，所以保证了长度和item一样。
		- |N(u)|表示用户u在隐式反馈特征中有行为的个数。
		- |y_i|表示用户u在隐式反馈上的表对将要评分item的隐性影响的权重数。可以理解为一种行为的数值化，如果没有就用0填充，保证长度一致；
	4. 举例说明：
		- 隐式反馈特征定义为：用户浏览过电影的电影介绍页的停留时间。
		- 用户u在这个隐式反馈特征上的表现必定不一样。故|N(u)|长度不一样。使用1/(|N(u)|)^-(1/2) 来平衡个数不同带来的影响。这种标准化的方法纯属是经验，无理论根据。
		- 举个例子：如果用户u对电影i有评分，且已知其在电影介绍页的停留时间，那么这种“修正方式”更能体现用户u对电影i的兴趣程度。
		- **可以认为，svd++是一种SVD 基于特征组合的扩展，也是对SVD 原始的latent factor的一种修正。

-- timeSVD++ ：http://10tiao.com/html/284/201509/207943847/1.html
	1. 玩法很简单，使用户过去的行为成为一个随时间衰减的变量，比如可以用Adam中用过的指数衰减法“EWMA(Exponentially Weighted Moving Average，指数加权移动平均) ”
	2. **timeSVD++其实是衰减过往用户行为的权重，提高就近用户隐式反馈行为的权重。

3、SVD中奇异值的物理意义：奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。回应了“机器学习的一个最根本也是最有趣的特性是数据压缩概念的相关性。”
- https://www.zhihu.com/question/22237507
- https://yanyiwu.com/work/2012/09/10/SVD-application-in-recsys.html


4、SVD主要是用于数据压缩和降噪，但也可以用于推荐系统，即将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。
- http://www.cnblogs.com/pinard/p/6251584.html
- 数据压缩便于存储；
- 通过SVD的矩阵压缩，便于存储；
- 通过SVD的矩阵分解，可以填充数据缺失的地方。
- 本质上，在用SVD之前，原始矩阵不允许出现缺失，如果有必须先用一些缺失值填充方法（均值填充或者随机数填充）进行填充后才能使用。这也就决定了传统的SVD的局限性。
- missing value被随机填充后的值，会在SVD完成后变成SVD对它的预测值。


2、SVD的参数依赖怎么解决？k要怎么取？能量计算？能量的计算：所有奇异值求平方和，累加到90%未止。？？？
3、NMF，全称为non-negative matrix factorization，中文呢为“非负矩阵分解”的玩法？
"""






























